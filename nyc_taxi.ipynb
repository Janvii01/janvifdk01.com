{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f78202",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16fd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffe0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"nyc_taxi_data_2014\").getOrCreate()\n",
    "df=spark.read.csv(r\"C:\\Users\\shuba\\Downloads\\nyc_taxi_data_2014.csv\\nyc_taxi_data_2014.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bf72ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partitioned = df.repartition(4)  # Adjust number of partitions based on data size\n",
    "\n",
    "# Display the schema\n",
    "df_partitioned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df50c25",
   "metadata": {},
   "source": [
    "Brief summary of each column in your dataset:\n",
    "\n",
    "1.vendor_id: Identifies the taxi vendor (company or fleet).\n",
    "2.pickup_datetime: Timestamp when the taxi trip started.\n",
    "3.dropoff_datetime: Timestamp when the taxi trip ended.\n",
    "4.passenger_count: The number of passengers in the taxi.\n",
    "5.trip_distance: The total distance of the trip in miles.\n",
    "6.pickup_longitude: Longitude coordinate where the trip started.\n",
    "7.pickup_latitude: Latitude coordinate where the trip started.\n",
    "8.rate_code: Code indicating the rate structure used for the fare.\n",
    "9.store_and_fwd_flag: Indicates if trip data was stored and forwarded.\n",
    "10.dropoff_longitude: Longitude coordinate where the trip ended.\n",
    "11.dropoff_latitude: Latitude coordinate where the trip ended.\n",
    "12.payment_type: The method used to pay for the trip (e.g., cash, credit card).\n",
    "13.fare_amount: Base fare for the trip before additional charges.\n",
    "14.surcharge: Extra charges added to the fare (e.g., tolls, taxes).\n",
    "15.mta_tax: The MTA tax applied to the trip.\n",
    "16.tip_amount: The tip given for the trip.\n",
    "17.tolls_amount: The tolls charged during the trip.\n",
    "18.total_amount: The total cost of the trip including all charges (fare, surcharges, tips, tolls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32cf8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+---------+------------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+---------+------------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|      CMT|2014-01-06 13:08:54|2014-01-06 13:11:36|              1|          0.4|      -73.981085|       40.73781|        1|                 N|       -73.985924|       40.736733|         CSH|        4.0|      0.0|    0.5|       0.0|         0.0|         4.5|\n",
      "|      CMT|2014-01-08 08:28:51|2014-01-08 08:48:50|              1|          2.5|      -73.859671|      40.684959|        1|                 N|       -73.859671|       40.684959|         CSH|       14.0|      0.0|    0.5|       0.0|         0.0|        14.5|\n",
      "|      CMT|2014-01-11 16:47:31|2014-01-11 17:01:18|              1|          2.4|      -73.971268|      40.760752|        1|                 N|       -73.994087|       40.738567|         CRD|       11.0|      0.0|    0.5|       1.0|         0.0|        12.5|\n",
      "|      CMT|2014-01-09 18:03:07|2014-01-09 18:04:54|              1|          0.6|      -73.949091|      40.777471|        1|                 N|       -73.943019|       40.785623|         CSH|        4.0|      1.0|    0.5|       0.0|         0.0|         5.5|\n",
      "|      CMT|2014-01-12 10:26:11|2014-01-12 10:42:10|              1|          8.6|      -73.964457|        40.7563|        1|                 N|       -73.872858|       40.774304|         CRD|       25.0|      0.0|    0.5|       5.1|         0.0|        30.6|\n",
      "|      CMT|2014-01-06 02:15:34|2014-01-06 02:25:09|              1|          2.3|       -73.97108|      40.755476|        1|                 N|       -73.992979|        40.74046|         CSH|       10.0|      0.5|    0.5|       0.0|         0.0|        11.0|\n",
      "|      CMT|2014-01-06 10:42:37|2014-01-06 10:47:22|              2|          0.9|             0.0|            0.0|        1|                 N|              0.0|             0.0|         CSH|        5.5|      0.0|    0.5|       0.0|         0.0|         6.0|\n",
      "|      CMT|2014-01-12 00:45:30|2014-01-12 00:50:50|              1|          1.0|      -73.989873|       40.73239|        1|                 N|       -73.980087|        40.74312|         CRD|        6.0|      0.5|    0.5|       1.4|         0.0|         8.4|\n",
      "|      CMT|2014-01-09 20:56:14|2014-01-09 21:02:41|              2|          1.8|      -73.987454|      40.748101|        1|                 N|        -73.98191|       40.770257|         CRD|        7.5|      0.5|    0.5|       1.7|         0.0|        10.2|\n",
      "|      CMT|2014-01-07 11:36:05|2014-01-07 11:37:40|              2|          0.2|       -73.99595|      40.760454|        1|                 N|       -74.000807|       40.762504|         CSH|        3.5|      0.0|    0.5|       0.0|         0.0|         4.0|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+---------+------------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partitioned.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eaf5873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows : 14999999\n"
     ]
    }
   ],
   "source": [
    "total_rows=df_partitioned.count()\n",
    "print(f\"Total Rows : {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526aeac",
   "metadata": {},
   "source": [
    "## Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6561b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Count:\n",
      "vendor_id: 0\n",
      "pickup_datetime: 0\n",
      "dropoff_datetime: 0\n",
      "passenger_count: 0\n",
      "trip_distance: 0\n",
      "pickup_longitude: 0\n",
      "pickup_latitude: 0\n",
      "rate_code: 0\n",
      "store_and_fwd_flag: 7636077\n",
      "dropoff_longitude: 145\n",
      "dropoff_latitude: 145\n",
      "payment_type: 0\n",
      "fare_amount: 0\n",
      "surcharge: 0\n",
      "mta_tax: 0\n",
      "tip_amount: 0\n",
      "tolls_amount: 0\n",
      "total_amount: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count missing values for each column and collect as dictionary\n",
    "missing_values = df.select(\n",
    "    [sum((col(column).isNull()).cast(\"int\")).alias(column) for column in df_partitioned.columns]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "# Print the missing values count for each column\n",
    "print(\"Missing Values Count:\")\n",
    "for column, count in missing_values.items():\n",
    "    print(f\"{column}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be812a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e96b5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any missing values (NULL or NaN) in any column\n",
    "df_cleaned = df_partitioned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12743278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Count:\n",
      "vendor_id: 0\n",
      "pickup_datetime: 0\n",
      "dropoff_datetime: 0\n",
      "passenger_count: 0\n",
      "trip_distance: 0\n",
      "pickup_longitude: 0\n",
      "pickup_latitude: 0\n",
      "rate_code: 0\n",
      "store_and_fwd_flag: 0\n",
      "dropoff_longitude: 0\n",
      "dropoff_latitude: 0\n",
      "payment_type: 0\n",
      "fare_amount: 0\n",
      "surcharge: 0\n",
      "mta_tax: 0\n",
      "tip_amount: 0\n",
      "tolls_amount: 0\n",
      "total_amount: 0\n"
     ]
    }
   ],
   "source": [
    "# Count missing values for each column and collect as dictionary\n",
    "missing_values = df_cleaned.select(\n",
    "    [sum((col(column).isNull()).cast(\"int\")).alias(column) for column in df_partitioned.columns]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "# Print the missing values count for each column\n",
    "print(\"Missing Values Count:\")\n",
    "for column, count in missing_values.items():\n",
    "    print(f\"{column}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb62168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|store_and_fwd_flag|\n",
      "+------------------+\n",
      "|                 Y|\n",
      "|                 N|\n",
      "+------------------+\n",
      "\n",
      "+------------+\n",
      "|payment_type|\n",
      "+------------+\n",
      "|         CSH|\n",
      "|         DIS|\n",
      "|         CRD|\n",
      "|         NOC|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique values in the 'store_and_fwd_flag' column\n",
    "df_cleaned.select('store_and_fwd_flag').distinct().show()\n",
    "\n",
    "# Get unique values in the 'payment_type' column\n",
    "df_cleaned.select('payment_type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a5d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|store_and_fwd_flag|\n",
      "+------------------+\n",
      "|                no|\n",
      "|               yes|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Replace 'Y' with 'yes' and 'N' with 'no' in the 'store_and_fwd_flag' column\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    'store_and_fwd_flag', \n",
    "    when(df_cleaned['store_and_fwd_flag'] == 'Y', 'yes')\n",
    "    .otherwise(when(df_cleaned['store_and_fwd_flag'] == 'N', 'no'))\n",
    ")\n",
    "\n",
    "# Show the updated column values\n",
    "df_cleaned.select('store_and_fwd_flag').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed9f830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|payment_type|\n",
      "+------------+\n",
      "|   No Charge|\n",
      "|      Credit|\n",
      "|    Discount|\n",
      "|        Cash|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Replace values in the 'payment_type' column\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"payment_type\",\n",
    "    F.when(df_cleaned[\"payment_type\"] == \"CSH\", \"Cash\")\n",
    "    .when(df_cleaned[\"payment_type\"] == \"DIS\", \"Discount\")\n",
    "    .when(df_cleaned[\"payment_type\"] == \"CRD\", \"Credit\")\n",
    "    .when(df_cleaned[\"payment_type\"] == \"NOC\", \"No Charge\")\n",
    "    .otherwise(df_cleaned[\"payment_type\"])  # Keep original value if not one of the above\n",
    ")\n",
    "\n",
    "# Show the updated values to verify\n",
    "df_cleaned.select(\"payment_type\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a662ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicate records\n",
    "duplicate_count = df_cleaned.count() - df_cleaned.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f760dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate records found. No rows removed.\n"
     ]
    }
   ],
   "source": [
    "if duplicate_count > 0:\n",
    "    # Remove duplicate records\n",
    "    df_cleaned = df_cleaned.dropDuplicates()\n",
    "    print(f\"Duplicate records handled: {duplicate_count} duplicates removed.\")\n",
    "else:\n",
    "    df_cleaned = df_cleaned\n",
    "    print(\"No duplicate records found. No rows removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82fbcd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Filtering Out Invalid Rows\n",
    "df_cleaned = df_cleaned.filter(df_cleaned['trip_distance'] > 0)\n",
    "df_cleaned = df_cleaned.filter(df_cleaned['fare_amount'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94254f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Split the Data into Training and Testing Sets\n",
    "train_set, test_set = df_cleaned.randomSplit([0.7, 0.3], seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea89450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+---------+------------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+---------+------------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|      CMT|2014-01-01 00:00:06|2014-01-01 00:10:11|              1|          6.0|      -73.870885|      40.773728|        1|                no|       -73.829123|       40.709735|        Cash|       18.0|      0.5|    0.5|       0.0|         0.0|        19.0|\n",
      "|      CMT|2014-01-01 00:00:06|2014-01-01 00:12:43|              1|          3.1|      -73.997171|      40.727207|        1|                no|       -73.964491|       40.753719|        Cash|       12.0|      0.5|    0.5|       0.0|         0.0|        13.0|\n",
      "|      CMT|2014-01-01 00:00:15|2014-01-01 00:09:53|              1|          1.4|       -73.96965|        40.7686|        1|                no|       -73.961928|       40.778783|        Cash|        9.0|      0.5|    0.5|       0.0|         0.0|        10.0|\n",
      "|      CMT|2014-01-01 00:00:22|2014-01-01 00:20:07|              1|          5.8|      -73.984835|       40.74546|        1|                no|       -73.911473|       40.744904|        Cash|       19.5|      0.5|    0.5|       0.0|         0.0|        20.5|\n",
      "|      CMT|2014-01-01 00:00:29|2014-01-01 00:04:06|              1|          0.3|      -73.988137|      40.744057|        1|                no|       -73.993086|       40.744785|        Cash|        4.0|      0.5|    0.5|       0.0|         0.0|         5.0|\n",
      "|      CMT|2014-01-01 00:00:36|2014-01-01 00:34:26|              2|         17.0|      -73.971757|      40.782268|        1|                no|       -73.736783|       40.723573|        Cash|       47.5|      0.5|    0.5|       0.0|         0.0|        48.5|\n",
      "|      CMT|2014-01-01 00:00:39|2014-01-01 00:01:38|              1|          0.3|      -73.973788|      40.751025|        1|                no|        -73.97133|       40.755053|        Cash|        3.0|      0.5|    0.5|       0.0|         0.0|         4.0|\n",
      "|      CMT|2014-01-01 00:00:43|2014-01-01 00:35:22|              1|          2.2|      -73.989456|      40.762791|        1|                no|       -73.976255|       40.744912|        Cash|       20.5|      0.5|    0.5|       0.0|         0.0|        21.5|\n",
      "|      CMT|2014-01-01 00:00:52|2014-01-01 00:06:06|              1|          1.4|      -73.976585|      40.682889|        1|                no|       -73.980684|       40.699652|        Cash|        7.0|      0.5|    0.5|       0.0|         0.0|         8.0|\n",
      "|      CMT|2014-01-01 00:00:52|2014-01-01 00:11:12|              1|          2.6|       -74.00004|       40.72874|        1|                no|       -73.972253|       40.747273|        Cash|       10.0|      0.5|    0.5|       0.0|         0.0|        11.0|\n",
      "|      CMT|2014-01-01 00:00:59|2014-01-01 00:34:53|              1|          3.3|      -73.977761|      40.763513|        1|                no|       -73.995506|       40.728202|        Cash|       21.5|      0.5|    0.5|       0.0|         0.0|        22.5|\n",
      "|      CMT|2014-01-01 00:01:04|2014-01-01 00:12:33|              3|          2.9|      -73.988082|      40.749887|        1|                no|       -74.004742|       40.737634|        Cash|       12.0|      0.5|    0.5|       0.0|         0.0|        13.0|\n",
      "|      CMT|2014-01-01 00:01:08|2014-01-01 00:18:58|              2|          3.1|      -73.980577|      40.759875|        1|                no|       -73.997628|       40.722867|        Cash|       14.5|      0.5|    0.5|       0.0|         0.0|        15.5|\n",
      "|      CMT|2014-01-01 00:01:11|2014-01-01 00:25:58|              2|         18.4|       -73.77672|      40.645081|        1|                no|       -73.843548|       40.852698|        Cash|       49.0|      0.5|    0.5|       0.0|        5.33|       55.33|\n",
      "|      CMT|2014-01-01 00:01:15|2014-01-01 00:03:03|              1|          0.2|      -73.977798|      40.763046|        1|                no|       -73.980331|       40.765688|        Cash|        3.5|      0.5|    0.5|       0.0|         0.0|         4.5|\n",
      "|      CMT|2014-01-01 00:01:24|2014-01-01 00:15:54|              1|          7.2|      -73.966474|      40.794462|        1|                no|       -73.981493|       40.714916|      Credit|       22.0|      0.5|    0.5|       1.5|         0.0|        24.5|\n",
      "|      CMT|2014-01-01 00:01:41|2014-01-01 00:05:07|              1|          0.5|      -74.006638|      40.731561|        1|                no|       -74.003806|       40.737577|        Cash|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|\n",
      "|      CMT|2014-01-01 00:01:41|2014-01-01 00:24:15|              3|          3.0|      -73.977803|      40.752263|        1|                no|       -74.007945|       40.740383|        Cash|       16.0|      0.5|    0.5|       0.0|         0.0|        17.0|\n",
      "|      CMT|2014-01-01 00:01:52|2014-01-01 00:14:30|              1|          3.9|      -73.989025|      40.764312|        1|                no|       -73.948838|        40.79452|        Cash|       13.5|      0.5|    0.5|       0.0|         0.0|        14.5|\n",
      "|      CMT|2014-01-01 00:01:52|2014-01-01 00:21:56|              1|          5.9|       -73.98333|      40.765889|        1|                no|       -73.886192|        40.74957|      Credit|       20.0|      0.5|    0.5|       4.2|         0.0|        25.2|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+---------+------------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393c13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assemble the features\n",
    "assembler = VectorAssembler(inputCols=[\"trip_distance\", \"fare_amount\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                                       \"dropoff_longitude\", \"dropoff_latitude\", \"surcharge\", \"mta_tax\", \n",
    "                                       \"tip_amount\", \"tolls_amount\", \"total_amount\"], outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df_cleaned)\n",
    "\n",
    "# Normalize using Min-Max Scaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561d269",
   "metadata": {},
   "source": [
    "## Data Analysis Using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d689bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.createOrReplaceTempView(\"taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90329971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------------+\n",
      "|         avg_fare|median_fare|      stddev_fare|\n",
      "+-----------------+-----------+-----------------+\n",
      "|11.84016095530172|        9.0|9.532408714536784|\n",
      "+-----------------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Aggregation: Summary statistics (mean, median, standard deviation)\n",
    "summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(fare_amount) AS avg_fare,\n",
    "        MEDIAN(fare_amount) AS median_fare,\n",
    "        STDDEV(fare_amount) AS stddev_fare\n",
    "    FROM taxi_data\n",
    "\"\"\")\n",
    "summary.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb922425",
   "metadata": {},
   "source": [
    "The table provides summary statistics for the fare_amount column in your dataset:\n",
    "\n",
    "Average Fare (avg_fare): The mean fare amount is approximately 11.84. This is the average value of all the fares in your dataset.\n",
    "\n",
    "Median Fare (median_fare): The median fare amount is 9.00. This means that half of the fares are below $9.00, and half are above. It's a measure of central tendency that is less sensitive to outliers compared to the average.\n",
    "\n",
    "Standard Deviation of Fare (stddev_fare): The standard deviation is approximately 9.53. This indicates the spread or variability in the fare amounts. A higher value suggests that the fares vary widely from the mean.\n",
    "\n",
    "Interpretation:\n",
    "The average fare is higher than the median fare, suggesting that there may be some higher fare outliers pulling the average up.\n",
    "The high standard deviation indicates that the fare amounts vary significantly, meaning there are both low and high fare trips in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97e0294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|payment_type|          total_fare|\n",
      "+------------+--------------------+\n",
      "|      Credit| 5.354956746000001E7|\n",
      "|        Cash|3.2626811770000003E7|\n",
      "|   No Charge|           302186.05|\n",
      "|    Discount|           113966.42|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Grouping and filtering: Group by 'payment_type' and calculate the total fare amount per payment type\n",
    "payment_grouped = spark.sql(\"\"\"\n",
    "    SELECT payment_type, SUM(fare_amount) AS total_fare\n",
    "    FROM taxi_data\n",
    "    GROUP BY payment_type\n",
    "    ORDER BY total_fare DESC\n",
    "\"\"\")\n",
    "payment_grouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c931a",
   "metadata": {},
   "source": [
    "This table shows the total fare amount (total_fare) for each payment_type in your dataset:\n",
    "\n",
    "Credit: The total fare amount for credit card payments is approximately 53.55 million. This suggests that the majority of transactions are paid using credit cards.\n",
    "\n",
    "Cash: The total fare amount for cash payments is about 32.63 million. While cash payments are still significant, they are less than credit card payments.\n",
    "\n",
    "No Charge: The total fare amount for no-charge transactions is 302,186.05. These transactions likely represent rides that were not charged, such as free rides or zero-fare promotions.\n",
    "\n",
    "Discount: The total fare amount for discounted transactions is 113,966.42. This is the smallest amount, indicating fewer discounted rides compared to the other payment types.\n",
    "\n",
    "Interpretation:\n",
    "Credit is the dominant payment method, with the highest total fare amount.\n",
    "Cash also makes up a significant portion of the total fare, but much less than credit.\n",
    "No Charge and Discount payments are relatively minor, which may indicate special cases, such as promotional rides or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c01b6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|day_of_week|          avg_fare|\n",
      "+-----------+------------------+\n",
      "|          1|11.678300983485261|\n",
      "|          2|11.981475070327736|\n",
      "|          3|11.884797757753606|\n",
      "|          4|12.040015236101613|\n",
      "|          5|12.043679669072347|\n",
      "|          6|12.016992251563929|\n",
      "|          7|11.162100062868168|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Time-based analysis: Group by day of the week and average fare amount\n",
    "time_based_analysis = spark.sql(\"\"\"\n",
    "    SELECT DAYOFWEEK(pickup_datetime) AS day_of_week, AVG(fare_amount) AS avg_fare\n",
    "    FROM taxi_data\n",
    "    GROUP BY day_of_week\n",
    "    ORDER BY day_of_week\n",
    "\"\"\")\n",
    "time_based_analysis.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b65c23",
   "metadata": {},
   "source": [
    "This table shows the average fare (avg_fare) for each day of the week (day_of_week) in your dataset:\n",
    "\n",
    "Day 1 (Sunday): The average fare is 11.68, which is the second-lowest among the days of the week.\n",
    "Day 2 (Monday): The average fare is 11.98, the highest on Mondays.\n",
    "Day 3 (Tuesday): The average fare is 11.88, showing a slight decrease compared to Monday.\n",
    "Day 4 (Wednesday): The average fare is 12.04, the highest value recorded, indicating slightly higher fares midweek.\n",
    "Day 5 (Thursday): The average fare is 12.04, tied with Wednesday for the highest fare.\n",
    "Day 6 (Friday): The average fare is 12.02, remaining high but slightly lower than Thursday.\n",
    "Day 7 (Saturday): The average fare is 11.16, the lowest among the days of the week, which might indicate more short trips or discounted rides during the weekend.\n",
    "Interpretation:\n",
    "Midweek (Wednesday and Thursday) shows the highest average fares, suggesting longer or more expensive trips.\n",
    "Sunday and Saturday have the lowest average fares, with Saturday being the least expensive day, possibly reflecting shorter or more routine trips.\n",
    "Monday also shows relatively high average fares, but not as high as the midweek days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3dd4b0",
   "metadata": {},
   "source": [
    "## Machine Learning Model (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f08c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90fddf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer for payment_type\n",
    "payment_type_encoder = StringIndexer(\n",
    "    inputCol=\"payment_type\", outputCol=\"encoded_payment_type\"\n",
    ")\n",
    "\n",
    "# StringIndexer for 'store_and_fwd_flag'\n",
    "store_and_fwd_flag_indexer = StringIndexer(\n",
    "    inputCol=\"store_and_fwd_flag\", outputCol=\"indexed_store_and_fwd_flag\"\n",
    ")\n",
    "\n",
    "# StringIndexer for 'vendor_id'\n",
    "vendor_id_indexer = StringIndexer(\n",
    "    inputCol=\"vendor_id\", outputCol=\"indexed_vendor_id\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d89a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features into a single vector using VectorAssembler\n",
    "feature_columns = [ 'passenger_count', 'trip_distance', 'pickup_longitude', \n",
    "    'pickup_latitude', 'rate_code','dropoff_longitude', 'dropoff_latitude', 'surcharge', 'mta_tax', \n",
    "    'tip_amount', 'tolls_amount', 'total_amount']\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns, outputCol=\"feature_vector1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a690578",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_assembled = assembler.transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22bc91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree Regressor model\n",
    "dt_regressor = DecisionTreeRegressor(featuresCol=\"feature_vector1\", labelCol=\"fare_amount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e0a3555",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_dt = Pipeline(stages=[feature_assembler, dt_regressor])\n",
    "model_dt = pipeline_dt.fit(train_set)\n",
    "predictions_dt = model_dt.transform(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dead972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|fare_amount|        prediction|\n",
      "+-----------+------------------+\n",
      "|       25.0|22.978673215898823|\n",
      "|        5.0|  4.80000631760609|\n",
      "|        3.5| 3.850213034118448|\n",
      "|        7.0| 6.423970538432056|\n",
      "|        7.5| 8.572593510026767|\n",
      "|       19.0|19.203894432932486|\n",
      "|        6.5|7.3112291635237465|\n",
      "|       17.0|15.905049095233503|\n",
      "|       10.5|11.259448323865549|\n",
      "|       20.5|18.896964048395727|\n",
      "+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show some predictions\n",
    "predictions_dt.select(\"fare_amount\", \"prediction\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87e8c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 4.364309227422931\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize the evaluator with the predicted and actual values for MSE\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "\n",
    "# Compute MSE\n",
    "mse = mse_evaluator.evaluate(predictions_dt)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c04f7",
   "metadata": {},
   "source": [
    "The Mean Squared Error (MSE) of 4.36 indicates the average squared difference between predicted and actual values. A lower MSE means better model performance. In this case, the MSE suggests reasonable predictions, but there is room for improvement. To improve accuracy, consider tuning the model or trying different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22883b9",
   "metadata": {},
   "source": [
    "## Model Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f81ec3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Step 1: Define a parameter grid for hyperparameter tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(dt_regressor.maxDepth, [5, 10, 15])\n",
    "    .addGrid(dt_regressor.maxBins, [32, 64, 128])\n",
    "    .addGrid(dt_regressor.minInstancesPerNode, [1, 5, 10])\n",
    "    .build())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ba75a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66773210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up Cross-Validation\n",
    "crossval = CrossValidator(estimator=pipeline_dt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  # 5-fold cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fit the model using cross-validation\n",
    "cv_model = crossval.fit(train_set)\n",
    "# Step 5: Get the best model from cross-validation\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Step 6: Evaluate the performance of the best model on the test set\n",
    "predictions_dt = best_model.transform(test_set)\n",
    "\n",
    "# Step 7: Compute Evaluation Metrics\n",
    "rmse = evaluator.evaluate(predictions_dt)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n",
    "\n",
    "# Optionally, you can compute additional evaluation metrics such as MSE or R2\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = mse_evaluator.evaluate(predictions_dt)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e2aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d1da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
